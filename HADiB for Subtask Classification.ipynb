{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Constants\n",
    "3. Turning off Randomness (optional)\n",
    "4. Data Loader\n",
    "5. Model Implementation\n",
    "    - Feature Extraction\n",
    "    - Hypergraph Construction\n",
    "    - ROI - ROI Message Passing\n",
    "    - Classification Head\n",
    "    - Building the model\n",
    "6. Training\n",
    "    - Compiling \n",
    "    - Visualising the computational graph\n",
    "    - Fitting the model\n",
    "    - Plotting Accuracy and Loss\n",
    "7. Testing\n",
    "8. Evaluation \n",
    "9. Training on different combinations of tasks and window sizes\n",
    "10. Inspecting the Output of the TemporalHypergraphLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_gnn as tfgnn\n",
    "import hypernetx as hnx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "Provided by [Neuromatch Academy](https://github.com/NeuromatchAcademy/course-content/blob/main/projects/fMRI/load_hcp_task.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The download cells will store the data in nested directories starting here:\n",
    "\n",
    "HCP_DIR = \"data/hcp/hcp_task\"\n",
    "# make the directory if it doesn't exist\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.makedirs(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated twice in each subject\n",
    "N_RUNS = 2\n",
    "\n",
    "# There are 7 tasks. Each has a number of 'conditions'\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR'      : {'runs': [5,6],   'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'runs': [7,8],   'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
    "    'EMOTION'    : {'runs': [9,10],  'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'runs': [11,12], 'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'runs': [13,14], 'cond':['math','story']},\n",
    "    'RELATIONAL' : {'runs': [15,16], 'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'runs': [17,18], 'cond':['mental','rnd']}\n",
    "}\n",
    "\n",
    "# Run mapping\n",
    "RUN_DIRECTION = {'RL': 0, 'LR': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning off Randomness\n",
    "This is optional. Uncomment for deterministic behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "\n",
    "# Set the seed for NumPy functions\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the seed for Python's built-in random module (optional, for other random functionalities you might use)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, batch_size, subjects, tasks, dir, run_direction, s_max, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialise the DataLoader. Loads the data and performs a train-val-test split.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of instances in a single batch.\n",
    "            subjects (list): List of subject indices.\n",
    "            tasks (list): List of task names.\n",
    "            dir (str): Path to the data directory.\n",
    "            run_direction (int): Run direction of the fMRI scan to be considerd, 'RL' or 'LR'.\n",
    "            s_max (int): Maximum window length.\n",
    "            random_state (int): Random seed, set for a deterministic train-val-test split.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.subjects = subjects\n",
    "        self.tasks = tasks\n",
    "        self.dir = dir\n",
    "        self.run_direction = run_direction\n",
    "        self.s_max = s_max\n",
    "        self.random_state = random_state\n",
    "        # Load and prepare data for all subjects and experiments\n",
    "        self.subjects_data, self.max_seq_len = self._load_and_prepare_data()\n",
    "        # Split data\n",
    "        self.train_data, self.train_labels, self.val_data, self.val_labels, self.test_data, self.test_labels = self._train_val_test_split(random_state=random_state)\n",
    "        self.all_labels = self.train_labels + self.val_labels + self.test_labels\n",
    "        self.unique_labels = list(set(self.train_labels) | set(self.val_labels) | set(self.test_labels))\n",
    "\n",
    "    def _encode_labels(self, y):\n",
    "        \"\"\"\n",
    "        Encode string labels to integers.\n",
    "\n",
    "        Args:\n",
    "            y (array): Array of string labels.\n",
    "\n",
    "        Returns:\n",
    "            array: Array of integer labels.\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        return y_encoded\n",
    "    \n",
    "    def pad_to_smax(self, instance):\n",
    "        \"\"\"\n",
    "        Pad the input sequence to the nearest multiple of s_max.\n",
    "\n",
    "        Args:\n",
    "            instance (array): Input sequence to be padded.\n",
    "\n",
    "        Returns:\n",
    "            array: Padded sequence.\n",
    "        \"\"\"\n",
    "        # Calculate the padding amount required\n",
    "        T = instance.shape[0]  # [time, features]\n",
    "        padding_needed = self.s_max - (T % self.s_max) if T % self.s_max != 0 else 0\n",
    "        # Apply padding\n",
    "        if padding_needed > 0:\n",
    "            padded_data = np.pad(instance, ((0, padding_needed), (0, 0)), 'constant', constant_values=0)\n",
    "        else:\n",
    "            padded_data = instance\n",
    "        return padded_data\n",
    "\n",
    "    def _load_and_prepare_data(self):\n",
    "        \"\"\"\n",
    "        Load and prepare data for all subjects and experiments.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing:\n",
    "                - tuple: Tuple containing:\n",
    "                    - list: List of all sequences.\n",
    "                    - array: Array of all labels.\n",
    "                - int: Maximum sequence length.\n",
    "        \"\"\"\n",
    "        all_X, all_y = [], []\n",
    "        max_seq_len = 0\n",
    "\n",
    "        for subject in self.subjects:\n",
    "            for experiment in self.tasks:\n",
    "                data = self._load_single_timeseries(subject, experiment, self.run_direction, self.dir, remove_mean=True)\n",
    "                evs = self._load_evs(subject, experiment, self.run_direction, self.dir)\n",
    "                X, y = self._prepare_sequential_data(data, evs, experiment)\n",
    "                \n",
    "                # Apply padding to each time series to the nearest multiple of s_max\n",
    "                X_padded = [self.pad_to_smax(x) for x in X]\n",
    "\n",
    "                all_X.extend(X_padded)  # Aggregate sequences\n",
    "                all_y.extend(y)  # Aggregate labels (condition indices)\n",
    "\n",
    "                # Update max sequence length if necessary\n",
    "                seq_lengths = [len(x) for x in X_padded]\n",
    "                if seq_lengths:\n",
    "                    max_seq_len = max(max_seq_len, max(seq_lengths))\n",
    "\n",
    "        # Encode string labels to integers\n",
    "        all_y_encoded = self._encode_labels(all_y)\n",
    "        return (all_X, np.array(all_y_encoded, dtype=int)), max_seq_len\n",
    "\n",
    "    def convert_labels(self, label_indices):\n",
    "        \"\"\"\n",
    "        Convert numeric predictions into original string labels.\n",
    "\n",
    "        Args:\n",
    "            label_indices (array): Array of label integers or softmax probabilities.\n",
    "\n",
    "        Returns:\n",
    "            array: Array of original string labels corresponding to the predictions.\n",
    "        \"\"\"\n",
    "        # If predictions are softmax probabilities, convert to integer labels first\n",
    "        if label_indices.ndim > 1 and label_indices.shape[1] > 1:\n",
    "            predicted_integers = label_indices.argmax(axis=1)\n",
    "        else:\n",
    "            predicted_integers = label_indices\n",
    "\n",
    "        # Use the label encoder to convert integer labels back to original strings\n",
    "        labels = self.label_encoder.inverse_transform(predicted_integers)\n",
    "        return labels\n",
    "    \n",
    "    def _prepare_sequential_data(self, data, evs, experiment):\n",
    "        \"\"\"\n",
    "        Split the time series for a single subject and experiment into segments based on the EVs.\n",
    "        \n",
    "        Args:\n",
    "            data (array): 2d array of time series data.\n",
    "            evs (list): List of frames associated with each condition.\n",
    "            experiment (str): Name of experiment.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Tuple containing:\n",
    "                - list: List of time series segments.\n",
    "                - array: Array of corresponding labels.\n",
    "        \"\"\"\n",
    "        X = []  # Stores the segments of time series data as independent arrays\n",
    "        y = []  # Stores the corresponding labels for the data\n",
    "\n",
    "        for cond_idx, cond in enumerate(EXPERIMENTS[experiment]['cond']):\n",
    "            for frame_group in evs[cond_idx]:\n",
    "                concatenated_frames = frame_group\n",
    "                if len(concatenated_frames) > 0 and max(concatenated_frames) < data.shape[1]:\n",
    "                    segment = np.concatenate([data[:, frame].reshape(-1, 1) for frame in concatenated_frames], axis=1)\n",
    "                    X.append(segment.T) # Transpose to ensure (time_steps, features) order\n",
    "                    unique_label = f\"{experiment}_{cond}\"\n",
    "                    y.append(unique_label)\n",
    "\n",
    "        y = np.array(y)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _load_single_timeseries(self, subject, experiment, run, dir, remove_mean=True):\n",
    "        \"\"\"\n",
    "        Load timeseries data for a single subject and single run.\n",
    "        Code provided by https://github.com/NeuromatchAcademy/course-content/blob/main/projects/fMRI/load_hcp_task.ipynb\n",
    "\n",
    "        Args:\n",
    "            subject (int): 0-based subject ID to load\n",
    "            experiment (str): Name of experiment\n",
    "            run (int): 0-based run index, across all tasks\n",
    "            remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "        Returns\n",
    "            ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "        \"\"\"\n",
    "\n",
    "        bold_run = EXPERIMENTS[experiment]['runs'][run]\n",
    "        bold_path = os.path.join(dir, \"subjects\", str(subject), \"timeseries\")\n",
    "        bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "        ts = np.load(os.path.join(bold_path, bold_file))\n",
    "        if remove_mean:\n",
    "            ts -= ts.mean(axis=1, keepdims=True)\n",
    "        return ts\n",
    "\n",
    "    def _load_evs(self, subject, experiment, run, dir):\n",
    "        \"\"\"\n",
    "        Load EVs (explanatory variables) data for one task experiment.\n",
    "        Code provided by https://github.com/NeuromatchAcademy/course-content/blob/main/projects/fMRI/load_hcp_task.ipynb\n",
    "        \n",
    "        Args:\n",
    "            subject (int): 0-based subject ID to load\n",
    "            experiment (str) : Name of experiment\n",
    "            run (int) : 0-based run index, across all tasks\n",
    "\n",
    "        Returns\n",
    "            evs (list of lists): A list of frames associated with each condition\n",
    "        \"\"\"\n",
    "\n",
    "        frames_list = []\n",
    "        task_key = 'tfMRI_' + experiment + '_' + ['RL', 'LR'][run]\n",
    "        for cond in EXPERIMENTS[experiment]['cond']:\n",
    "            ev_file = os.path.join(dir, \"subjects\", str(subject), \"EVs\",\n",
    "                                str(task_key), f\"{cond}.txt\")\n",
    "\n",
    "            ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "            ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "            # Determine when trial starts, rounded down\n",
    "            start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "            # Use trial duration to determine how many frames to include for trial\n",
    "            duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "            # Take the range of frames that correspond to this specific trial\n",
    "            frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "            frames_list.append(frames)\n",
    "\n",
    "        return frames_list\n",
    "\n",
    "    def _train_val_test_split(self, val_size=0.2, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Perform a train-val-test split on the data. Test size = 1 - (val_size + train_size).\n",
    "\n",
    "        Args:\n",
    "            val_size (float): Proportion of the data to include in the validation split.\n",
    "            test_size (float): Proportion of the data to include in the test split.\n",
    "            random_state (int): Random seed, set for a deterministic train-val-test split.\n",
    "        \n",
    "        Returns:\n",
    "            tuple containing the train, validation, and test data and labels.\n",
    "        \"\"\"\n",
    "        all_X, all_y = self.subjects_data\n",
    "        indices = list(range(len(all_X)))  # Get a list of indices\n",
    "        \n",
    "        # Split indices to maintain correspondence between data and labels\n",
    "        train_indices, temp_indices = train_test_split(indices, test_size=test_size + val_size, random_state=random_state)\n",
    "        val_indices, test_indices = train_test_split(temp_indices, test_size=test_size / (test_size + val_size), random_state=random_state)\n",
    "\n",
    "        # Use list comprehension to select the data based on the indices\n",
    "        train_data = [all_X[idx] for idx in train_indices]\n",
    "        train_labels = [all_y[idx] for idx in train_indices]\n",
    "\n",
    "        val_data = [all_X[idx] for idx in val_indices]\n",
    "        val_labels = [all_y[idx] for idx in val_indices]\n",
    "\n",
    "        test_data = [all_X[idx] for idx in test_indices]\n",
    "        test_labels = [all_y[idx] for idx in test_indices]\n",
    "\n",
    "        return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "    def _make_dataset(self, data, labels):\n",
    "        \"\"\"\n",
    "        Create a tf.data.Dataset from the data and labels.\n",
    "        \n",
    "        Args:\n",
    "            data (list): List of sequences.\n",
    "            labels (array): Array of labels.\n",
    "\n",
    "        Returns:\n",
    "            tf.data.Dataset: Dataset containing the data and labels.\n",
    "        \"\"\"\n",
    "        # Convert the lists of arrays into a format suitable for tf.data.Dataset\n",
    "        def gen():\n",
    "            for seq, label in zip(data, labels):\n",
    "                yield seq, label\n",
    "        \n",
    "        # Specify output types; sequences are float32 and labels are int32\n",
    "        output_types = (tf.float32, tf.int32)\n",
    "        \n",
    "        # As sequences have variable lengths, use None for the time dimension\n",
    "        output_shapes = (tf.TensorShape([None, data[0].shape[1]]), tf.TensorShape([]))\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_generator(generator=gen, \n",
    "                                                output_types=output_types, \n",
    "                                                output_shapes=output_shapes)\n",
    "        \n",
    "        # Apply dynamic padding\n",
    "        # TensorFlow will automatically pad the sequences to match the longest sequence in each batch\n",
    "        dataset = dataset.padded_batch(batch_size=self.batch_size, \n",
    "                                    padded_shapes=([None, None], []),  # Automatically pad all dimensions except the batch dimension\n",
    "                                    padding_values=(0.0, tf.constant(-1, dtype=tf.int32)))  # -1 for labels as a placeholder. Labels are not padded, but tf requires a parameter.\n",
    "        return dataset\n",
    "\n",
    "    def get_train(self):\n",
    "        return self._make_dataset(data=self.train_data, labels=self.train_labels)\n",
    "\n",
    "    def get_val(self):\n",
    "        return self._make_dataset(self.val_data, self.val_labels)\n",
    "\n",
    "    def get_test(self):\n",
    "        return self._make_dataset(self.test_data, self.test_labels)\n",
    "    \n",
    "    def get_all(self):\n",
    "        data, labels = self.subjects_data\n",
    "        return self._make_dataset(data, labels)\n",
    "\n",
    "    def get_all_X_y(self):\n",
    "        return self.subjects_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['GAMBLING', 'SOCIAL', 'RELATIONAL', 'WM', 'MOTOR', 'EMOTION']\n",
    "subjects = range(N_SUBJECTS)\n",
    "dir = HCP_DIR\n",
    "run_direction = RUN_DIRECTION['RL']\n",
    "random_state = 42\n",
    "S_MAX = 20 # sequences are padded to the nearest multiple of s_max to avoid dynamic padding in the model\n",
    "data_loader = DataLoader(batch_size=16, tasks=tasks, subjects=subjects, dir=dir, run_direction=run_direction, s_max=S_MAX, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to verify the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of each label\n",
    "label_counts = {}\n",
    "for label in data_loader.unique_labels:\n",
    "    label_counts[label] = 0\n",
    "\n",
    "for label in data_loader.all_labels:\n",
    "    label_counts[label] += 1\n",
    "print(label_counts)\n",
    "print(data_loader.convert_labels(np.array(data_loader.unique_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(inputs, W1_time, W2_time, W1_roi, W2_roi):\n",
    "    \"\"\" \n",
    "    Residual block with time and ROI mixing.\n",
    "    The layers are shared for processing inputs of the same shape (i.e. the same number of time steps)\n",
    "    Args:\n",
    "        inputs (tensor): Input tensor. Shape [Batch, Time, ROI]\n",
    "        W1_time (layer): Dense layer for time mixing\n",
    "        W2_time (layer): Dense layer for time mixing\n",
    "        W1_roi (layer): Dense layer for ROI mixing\n",
    "        W2_roi (layer): Dense layer for ROI mixing\n",
    "    \n",
    "    Returns:\n",
    "        tensor: With time and roi-mixing applied. Shape [Batch, Time, ROI]\n",
    "    \"\"\"\n",
    "\n",
    "    layer_norm_time = layers.LayerNormalization(axis=-1) \n",
    "    layer_norm_roi = layers.LayerNormalization(axis=-1)\n",
    "    \n",
    "    # Time mixing\n",
    "    x = tf.transpose(inputs, perm=[0, 2, 1])  # Transpose for time mixing\n",
    "    x = layer_norm_time(x)\n",
    "    \n",
    "    x = W1_time(x)\n",
    "    x = W2_time(x)\n",
    "    \n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # Transpose back\n",
    "    x = layers.Dropout(0.7)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # ROI mixing\n",
    "    x = layer_norm_roi(res)\n",
    "    \n",
    "    x = W1_roi(x)\n",
    "    x = layers.Dropout(0.7)(x)\n",
    "    x = W2_roi(x)\n",
    "    x = layers.Dropout(0.7)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_layers(sizes, ff_dim, n_rois):\n",
    "    \"\"\"\n",
    "    Creates shared Dense layers for each size with distinct layers for time and ROI processing.\n",
    "    \n",
    "    Args:\n",
    "        sizes: A list of sizes for time mixing. These also serve as the input/output sizes for ROI mixing.\n",
    "        ff_dim: The fixed feature dimension size for the first step of ROI mixing.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary of shared layers, keyed by size, with each entry containing four Dense layers.\n",
    "    \"\"\"\n",
    "    # TODO: If want to use more than one block, we need to create a separate set of layers for each block.\n",
    "    shared_layers = {}\n",
    "    for size in sizes:\n",
    "        shared_layers[size] = {\n",
    "            'W1_time': layers.Dense(size, activation='gelu', name=f'W1_time_{size}'),\n",
    "            'W2_time': layers.Dense(size, activation=None, name=f'W2_time_{size}'),\n",
    "            # For W1_roi, use ff_dim as the output size, transformation to a fixed feature space.\n",
    "            'W1_roi': layers.Dense(ff_dim, activation='gelu', name=f'W1_roi_{size}'), # ff_dim\n",
    "            # W2_roi should match the original input's last dimension, which we assume is represented by `size` here.\n",
    "            'W2_roi': layers.Dense(n_rois, activation=None, name=f'W2_roi_{size}'),\n",
    "        }\n",
    "    return shared_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_fixed_time(x_s, fixed_time):\n",
    "    \"\"\"\n",
    "    Projects the time series x_s to a fixed time dimension.\n",
    "\n",
    "    Args:\n",
    "        x_s: The input time series tensor. Shape [Batch, Time, ROI]\n",
    "        fixed_time: The fixed time dimension to project to.\n",
    "        \n",
    "    Returns:\n",
    "        The projected tensor. Shape [Batch, fixed_time, ROI]\n",
    "    \"\"\"\n",
    "    x_s = tf.transpose(x_s, perm=[0, 2, 1])  # [Batch, ROIs, Time]\n",
    "    x_s = layers.Dense(fixed_time)(x_s)      # [Batch, ROIs, fixed_time]\n",
    "    x_s = tf.transpose(x_s, perm=[0, 2, 1])  # [Batch, fixed_time, ROIs]\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(input, S, ff_dim, n_block=1):\n",
    "    \"\"\" \n",
    "    Hierarchical time series encoder for multiple scales.\n",
    "    \n",
    "    Args:\n",
    "        input (tensor): Input tensor. Shape [Batch, Time, ROI]\n",
    "        S (list): List of window sizes for time segmentation\n",
    "        ff_dim (int): Fixed feature dimension size for the first step of ROI mixing\n",
    "        n_block (int): Number of blocks to use in the model\n",
    "\n",
    "    Returns:\n",
    "        tensor: Feature representation of the input. Shape [Batch, Segments, Time, ROI]\n",
    "    \"\"\"\n",
    "    s_max = max(S)\n",
    "    shared_layers = create_shared_layers(S, ff_dim, input.shape[-1])\n",
    "\n",
    "    H_F_ts = []  # Stores the combined feature representation for all s_max segments\n",
    "\n",
    "    T = input.shape[-2]  # Length of the time series\n",
    "    num_segments = T // s_max + (1 if T % s_max else 0)  # Number of s_max segments\n",
    "\n",
    "    for t in range(num_segments):\n",
    "        end_idx = min((t + 1) * s_max, T) # End index for the current s_max segment\n",
    "\n",
    "        scale_features = [] # To store features from all scales within the s_max segment\n",
    "        for s in S:\n",
    "            # Create sliding windows of size 's' within the current s_max segment\n",
    "            for start_sub_t in range(0, end_idx, s):\n",
    "                sub_end_idx = min(start_sub_t + s, end_idx) # this would cut off the last segment if it's smaller than s, however it will not happen as the window sizes are required to be factors of s_max and each other.\n",
    "                sub_segment = input[:, start_sub_t:sub_end_idx, :]  # Slice the sub-segment\n",
    "\n",
    "                for _ in range(n_block):\n",
    "                    # Retrieve shared layers for the current size\n",
    "                    layers_for_s = shared_layers[s]\n",
    "                    sub_segment = res_block(\n",
    "                        sub_segment,\n",
    "                        layers_for_s['W1_time'], layers_for_s['W2_time'],\n",
    "                        layers_for_s['W1_roi'], layers_for_s['W2_roi']\n",
    "                    )\n",
    "\n",
    "                # Project the extracted features to a fixed size\n",
    "                sub_segment = project_to_fixed_time(sub_segment, T) # [Batch, Full Time, ROIs]\n",
    "\n",
    "                scale_features.append(sub_segment)\n",
    "\n",
    "        # Combine features from all scales within the s_max segment\n",
    "        H_F_t = layers.Concatenate(axis=-1)(scale_features) # [Batch, Time, ROIs x nr of segments]\n",
    "\n",
    "        # Note: This operation is ambiguous in the paper by Behrouz et al. Refer to section 5.2.4 Point 3 in report.\n",
    "        H_F_t = layers.Dense(input.shape[-1], activation='gelu')(H_F_t) # [Batch, Time, ROIs]\n",
    "\n",
    "        H_F_ts.append(H_F_t)\n",
    "   \n",
    "    # stack along the new axis=1 to get [Batch, Segments, Time, Features]\n",
    "    H_F_ts_stacked = tf.stack(H_F_ts, axis=1)\n",
    "    \n",
    "    return H_F_ts_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypergraphConstructionLayer(tf.keras.layers.Layer):\n",
    "    \"\"\" \n",
    "    Uses a modified self-attention mechanism to construct a temporal hypergraph from the mixed time series. \n",
    "    Outputs attention scores which translate to the incidence matrix A(t) of the hypergraph representing the segment h_f_t.\n",
    "    \"\"\"\n",
    "    def __init__(self, roi_dim, hyperedge_dim, attn_dim, **kwargs):\n",
    "        \"\"\" \n",
    "        Initialises the HypergraphConstructionLayer.\n",
    "\n",
    "        Args:\n",
    "            roi_dim (int): The number of the ROIs in the input time series.\n",
    "            hyperedge_dim (int): The number of time steps in the input, equivalent to the number of hyperedges in the hypergraph.\n",
    "            attn_dim (int): The size of the attention mechanism vector.\n",
    "        \"\"\"\n",
    "        super(HypergraphConstructionLayer, self).__init__(**kwargs)\n",
    "        self.roi_dim = roi_dim\n",
    "        self.hyperedge_dim = hyperedge_dim  # previously time_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.w_q = self.add_weight(shape=(roi_dim, attn_dim),\n",
    "                                   initializer='random_normal',\n",
    "                                   trainable=True,\n",
    "                                   name='w_q')\n",
    "        self.w_k = self.add_weight(shape=(hyperedge_dim, attn_dim),\n",
    "                                   initializer='random_normal',\n",
    "                                   trainable=True,\n",
    "                                   name='w_k')\n",
    "        \n",
    "\n",
    "    def call(self, h_f_t):\n",
    "        \"\"\" \n",
    "        Constructs the temporal hypergraph for the segment h_f_t.\n",
    "\n",
    "        Args:\n",
    "            h_f_t (tensor): The mixed time series segment. Shape [Batch, Time, ROIs]. Time is transformed to hyperedges.\n",
    "\n",
    "        Returns:\n",
    "            tensor: The incidence matrix A(t) of the temporal hypergraph. Shape [Batch, ROIs, Hyperedges].\n",
    "        \"\"\"\n",
    "        q_t = tf.matmul(h_f_t, self.w_q)  # [Batch, Hyperedges, Attention Dimension]\n",
    "        h_f_t_transposed = tf.transpose(h_f_t, perm=[0, 2, 1])  # [Batch, ROIs, Hyperedges]\n",
    "        k_t = tf.matmul(h_f_t_transposed, self.w_k)  # [Batch, ROIs, Attention Dimension]\n",
    "\n",
    "        # Transpose k_t to match the dimensions expected for the attention mechanism\n",
    "        k_t_transposed = tf.transpose(k_t, perm=[0, 2, 1])  # [Batch, Attention Dimension, Hyperedges]\n",
    "\n",
    "        # Compute the normalising denominator\n",
    "        norm_factor = tf.sqrt(tf.cast(self.attn_dim, tf.float32)) # Normalisation factor ambiguous in the paper by Behrouz et al. Refer to section 5.3.2 in report.\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        attn_logits = tf.matmul(q_t, k_t_transposed) / norm_factor  # [Batch, Hyperedges, ROIs]\n",
    "        attn_scores = tf.sigmoid(attn_logits)  # Sigmoid to get scores in range [0, 1]\n",
    "\n",
    "        # Transpose the attention scores to get the desired shape [Batch, ROIs, Hyperedges]\n",
    "        attn_scores_transposed = tf.transpose(attn_scores, perm=[0, 2, 1])\n",
    "\n",
    "        return attn_scores_transposed\n",
    "\n",
    "class TemporalHypergraphLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer to construct a temporal hypergraph for each time segment.\n",
    "    \"\"\"\n",
    "    def __init__(self, hyperedge_dim, roi_dim, attn_dim, n_hypergraphs, **kwargs):\n",
    "        super(TemporalHypergraphLayer, self).__init__(**kwargs)\n",
    "        self.hyperedge_dim = hyperedge_dim\n",
    "        self.roi_dim = roi_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.n_hypergraphs = n_hypergraphs\n",
    "        self.thg_construction_layer = HypergraphConstructionLayer(\n",
    "            hyperedge_dim=hyperedge_dim,\n",
    "            roi_dim=roi_dim,\n",
    "            attn_dim=attn_dim\n",
    "        )\n",
    "\n",
    "    def call(self, h_f_ts):\n",
    "        \"\"\" \n",
    "        Constructs a hyergraph for each time segment in h_f_ts, forming a temporal hypergraph.\n",
    "\n",
    "        Args:\n",
    "            h_f_ts (tensor): The mixed time series segments. Shape [Batch, Segments, Time, ROIs].\n",
    "            \n",
    "        Returns:\n",
    "            tensor: The adjacency matrices for the temporal hypergraphs. Shape [Batch, Segments, ROIs, Hyperedges].\n",
    "        \"\"\"\n",
    "        # Unstack the segments dimension to process each time step independently\n",
    "        time_segments = tf.unstack(h_f_ts, num=self.n_hypergraphs, axis=1)\n",
    "\n",
    "        # Apply the hypergraph construction layer to each time segment\n",
    "        A_ts_slices = [self.thg_construction_layer(time_slice) for time_slice in time_segments]\n",
    "\n",
    "        # Stack the resulting adjacency matrices along the time dimension\n",
    "        A_ts = tf.stack(A_ts_slices, axis=1)  # [Batch, Segment, ROI, Hyperedge]\n",
    "\n",
    "        return A_ts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def incidence_matrix_to_graph_tensor_with_roi_features(incidence_matrix, \n",
    "                                                       roi_nodes_initial_state, \n",
    "                                                       roi_node_set_name,\n",
    "                                                       he_node_set_name,\n",
    "                                                       hidden_state_name,\n",
    "                                                       edges_name,\n",
    "                                                       hyperedge_nodes_initial_state=None):\n",
    "    \"\"\"\n",
    "    Converts an incidence matrix to a GraphTensor representing a heterogeneous bipartite graph with different types of nodes for rois and hyperedges.\n",
    "\n",
    "    Args:\n",
    "        incidence_matrix (tf.Tensor): The incidence matrix where rows represent nodes and columns represent edges.\n",
    "        roi_nodes_initial_state (tf.Tensor): The initial encoding for the ROI nodes. Needs to be of the size [num_rois, encoding_size], where num_rois is incidence_matrix.shape[0].\n",
    "        hyperedge_nodes_initial_state (tf.Tensor): The initial encoding for the hyperedge nodes. Needs to be of the size [num_hyperedges, encoding_size], where num_hyperedges is incidence_matrix.shape[1]. \n",
    "                                                   This is currently not used but would enhance ROI-ROI message passing by providing the hyperedge encodings.\n",
    "\n",
    "    Returns:\n",
    "        A tfgnn.GraphTensor object representing the bipartite graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of row and column nodes\n",
    "    num_row_nodes = incidence_matrix.shape[0]\n",
    "    num_col_nodes = incidence_matrix.shape[1]\n",
    "\n",
    "    # Create node sets for rows and columns\n",
    "    roi_nodes = tfgnn.NodeSet.from_fields(\n",
    "        sizes=tf.constant([num_row_nodes]), \n",
    "        features={hidden_state_name: roi_nodes_initial_state}\n",
    "\n",
    "    )\n",
    "    he_nodes = tfgnn.NodeSet.from_fields(\n",
    "        sizes=tf.constant([num_col_nodes]), \n",
    "        # features={'hidden_state': hyperedge_nodes_initial_state}\n",
    "        features={}\n",
    "    )\n",
    "\n",
    "    # Create edges from non-zero entries of the incidence matrix\n",
    "    nonzero_indices = tf.where(tf.reshape(incidence_matrix, [-1]) > 0)\n",
    "\n",
    "    # Compute source (row) and target (column) indices for edges (this is used since unravel_index not available in TensorFlow Keras functional API)\n",
    "    # Flatten the incidence matrix and get indices of non-zero entries\n",
    "    flat_nonzero_indices = nonzero_indices[:, 0]\n",
    "    # Compute row indices: divide the flat indices by the number of columns\n",
    "    row_indices = flat_nonzero_indices // num_col_nodes\n",
    "    # Compute column indices: get the remainder of the division\n",
    "    col_indices = flat_nonzero_indices % num_col_nodes # Not differentiable but that is fine in this contex. \n",
    "\n",
    "    edge_sizes = tf.reshape(tf.size(row_indices), [1])\n",
    "    edges = tfgnn.EdgeSet.from_fields(\n",
    "        sizes=edge_sizes,\n",
    "        adjacency=tfgnn.Adjacency.from_indices(\n",
    "            source=(roi_node_set_name, row_indices),\n",
    "            target=(he_node_set_name, col_indices)\n",
    "        ),\n",
    "        features={}\n",
    "    )\n",
    "\n",
    "    # Create the bipartite graph\n",
    "    graph_tensor = tfgnn.GraphTensor.from_pieces(\n",
    "        node_sets={roi_node_set_name: roi_nodes, he_node_set_name: he_nodes},\n",
    "        edge_sets={edges_name: edges}\n",
    "    )\n",
    "\n",
    "    return graph_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_roi_message_passing(graph, roi_node_set_name, he_node_set_name, hidden_state_name, edges_name):\n",
    "    \"\"\"Performs two passes of message passing in a bipartite hypergraph representation.\n",
    "    1. From roi_nodes to he_nodes (hyperedges).\n",
    "    2. From he_nodes (hyperedges) back to roi_nodes.\n",
    "    This accumulates the messages from all neighbours of a node in the hypergraph representation.\n",
    "    \n",
    "    Args:\n",
    "        graph (tfgnn.GraphTensor):\n",
    "        roi_node_set_name (string): The name of the nodes in the graph representing ROIs.\n",
    "        he_node_set_name (string): The name of the nodes in the graph representing Hyperedges.\n",
    "        edge_set_name (string): The name of edge set name. In the bipartite graph there is only one type and defaults to tfgnn.EDGES constant.\n",
    "        hidden_state_name (string): The name of the hidden state feature in the node sets.\n",
    "\n",
    "    Returns:\n",
    "        tfgnn.GraphTensor: The updated graph tensor with the ROI-ROI message passing applied.\n",
    "    \"\"\"\n",
    "    # First pass: roi_nodes to he_nodes\n",
    "    sender_features_to_he = tfgnn.broadcast_node_to_edges(\n",
    "        graph, edges_name, tfgnn.SOURCE, feature_name=hidden_state_name)\n",
    "    he_new_features = tfgnn.pool_edges_to_node(\n",
    "        graph, edges_name, tfgnn.TARGET, reduce_type=\"sum\",\n",
    "        feature_value=sender_features_to_he)\n",
    "    \n",
    "    # Update he_node features with the result of the first pass\n",
    "    he_updated_features = {he_node_set_name: {hidden_state_name: he_new_features}}\n",
    "    graph = graph.replace_features(node_sets=he_updated_features)\n",
    "\n",
    "    # Second pass: he_nodes back to roi_nodes\n",
    "    sender_features_to_roi = tfgnn.broadcast_node_to_edges(\n",
    "        graph, edges_name, tfgnn.TARGET, feature_name=hidden_state_name)\n",
    "    roi_new_features = tfgnn.pool_edges_to_node(\n",
    "        graph, edges_name, tfgnn.SOURCE, reduce_type=\"sum\",\n",
    "        feature_value=sender_features_to_roi)\n",
    "    \n",
    "    # Get original roi_node features for skip connection\n",
    "    original_roi_node_features = graph.node_sets[roi_node_set_name].features[hidden_state_name]\n",
    "\n",
    "    # Add (sum) the original roi_node features to the new roi_node features for skip connection\n",
    "    updated_roi_node_features = roi_new_features + original_roi_node_features\n",
    "\n",
    "    # Update roi_node features with the result of the second pass\n",
    "    roi_updated_features = {roi_node_set_name: {hidden_state_name: updated_roi_node_features}}\n",
    "    updated_graph = graph.replace_features(node_sets=roi_updated_features)\n",
    "\n",
    "    return updated_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    A layer that converts an Incidence Matrix to a tf-gnn GraphTensor in the form of a bipartite graph and performs ROI-ROI and ROI-Hyperedge message passing.\n",
    "    \"\"\"\n",
    "    def __init__(self, message_passing_layers=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialises the MessagePassingLayer.\n",
    "        Args:\n",
    "            message_passing_layers (int): The number of message passing layers to use.\n",
    "        \"\"\"\n",
    "        super(MessagePassingLayer, self).__init__(**kwargs)\n",
    "        self.message_passing_layers = message_passing_layers\n",
    "        # Constants for naming nodes in the bipartite graph\n",
    "        self.ROI_NODES_NAME = 'roi_nodes'\n",
    "        self.HE_NODES_NAME = 'he_nodes'\n",
    "        self.HIDDEN_STATE_NAME = 'hidden_state'\n",
    "        self.EDGES_NAME = tfgnn.EDGES\n",
    "\n",
    "    def call(self, A_ts, H_F_ts_T):\n",
    "        \"\"\"\n",
    "        Forward pass for the MessagePassingLayer. Converts the incidence matrix to a GraphTensor and performs message passing.\n",
    "\n",
    "        Args:\n",
    "            A_ts (tensor): The incidence matrix for the temporal hypergraph. Shape [Batch, Segments, Node, Hyperedge].\n",
    "            H_F_ts_T (tensor): The mixed time series segments, transposed to be of shape [Batch, Segments, Time, ROIs].\n",
    "\n",
    "        Returns:\n",
    "            tensor: The pooled features after message passing. Shape [Batch, ROIs, Features].\n",
    "        \"\"\"\n",
    "        def process_time_step(args):\n",
    "            # A_t shape: [ROI, Hyperedge]\n",
    "            # roi_nodes_initial_state shape: [ROI, Feature_Dimension]\n",
    "\n",
    "            A_t, roi_nodes_initial_state = args\n",
    "            # Convert each slice into a GraphTensor\n",
    "\n",
    "            # Convert single hypergraph into Bipartite Graph\n",
    "            graph = incidence_matrix_to_graph_tensor_with_roi_features(incidence_matrix=A_t, \n",
    "                                                                       roi_nodes_initial_state=roi_nodes_initial_state, \n",
    "                                                                       roi_node_set_name=self.ROI_NODES_NAME, \n",
    "                                                                       he_node_set_name=self.HE_NODES_NAME,\n",
    "                                                                       hidden_state_name=self.HIDDEN_STATE_NAME,\n",
    "                                                                       edges_name=self.EDGES_NAME) \n",
    "            \n",
    "            # Apply message passing\n",
    "            graph = roi_roi_message_passing(graph, \n",
    "                                            roi_node_set_name=self.ROI_NODES_NAME, \n",
    "                                            he_node_set_name=self.HE_NODES_NAME,\n",
    "                                            hidden_state_name=self.HIDDEN_STATE_NAME,\n",
    "                                            edges_name=self.EDGES_NAME)\n",
    "            \n",
    "            # Extract updated roi_nodes features and return\n",
    "            return graph.node_sets[self.ROI_NODES_NAME].features[self.HIDDEN_STATE_NAME]\n",
    "\n",
    "        def process_instance(args):\n",
    "            \"\"\"\n",
    "            Process a single instance in the batch.\n",
    "\n",
    "            Args:\n",
    "                args: A tuple of A_ts_slice and H_F_ts_T_slice.\n",
    "                \n",
    "            Returns:\n",
    "                The pooled features for the instance.\n",
    "            \"\"\"\n",
    "            # A_ts_slice shape: [Time, ROI, Hyperedge]\n",
    "            # H_F_ts_T_slice shape: [Time, ROI, Feature_Dimension]\n",
    "            A_ts_slice, H_F_ts_T_slice = args\n",
    "\n",
    "            # could use GRU here to combine the time steps (somehow)\n",
    "\n",
    "            # Use tf.map_fn to process each time step\n",
    "            processed_features = tf.map_fn(process_time_step, (A_ts_slice, H_F_ts_T_slice), fn_output_signature=tf.float32)\n",
    "            # Reduce mean across time steps (axis=0)\n",
    "            return tf.reduce_mean(processed_features, axis=0)\n",
    "\n",
    "        # Use tf.map_fn to process each batch\n",
    "        # A_ts shape: [Batch, Time, ROI, Hyperedge]\n",
    "        # H_F_ts_T shape: [Batch, Time, ROI, Feature_Dimension]\n",
    "        pooled_features = tf.map_fn(process_instance, (A_ts, H_F_ts_T), fn_output_signature=tf.float32)\n",
    "        \n",
    "        return pooled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_head(input, n_classes):\n",
    "    \"\"\"\n",
    "    Classification head to output membership probabilities for each class.\n",
    "\n",
    "    Args:\n",
    "        input (tf.Tensor): Input tensor for the classification head.\n",
    "        n_classes (int): Number of classes for the output layer.\n",
    "\n",
    "    Returns:\n",
    "        The output tensor of the classification head with softmax activation. Shape [Batch, n_classes].\n",
    "    \"\"\"\n",
    "    # x = layers.GlobalAveragePooling1D()(input)\n",
    "    x = layers.Flatten()(input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, S, ff_dim, n_classes, n_block=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_shape:\n",
    "        S (list): A list of all window sizes.\n",
    "        n_block (int): N-blocks the the number of repetitions of the residual block in feature extraction. This is an artefact from the TSM-Mixer. HADIB uses a single residual block and therefore it defaults to 1.\n",
    "        ff_dim (int): A constant used in Feature Extraction and classification head to specify the size of single layers that need to be constant.\n",
    "        n_classes (int): The number of classes of the multiclass-classification problem.\n",
    "    Returns:\n",
    "        tf.keras.models.Model. Generated model graph depends on the input parameters.\n",
    "    \"\"\"\n",
    "    input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Masking layer immediately after the Input to ignore the padded values\n",
    "    x = layers.Masking(mask_value=0.0)(input) # [Batch, Time, ROIs]\n",
    "    \n",
    "    # Extract hierarchical time series\n",
    "    H_F_ts = feature_extraction(input=x, S=S, n_block=n_block, ff_dim=ff_dim) # [Batch, Segments, Time, ROIs], e.g. [None, 2, 40, 360] for s_max=20 and t=40\n",
    "\n",
    "    # Construct temporal hypergraph for each H_F_t (Incidence matrices from time segments H_F_t)\n",
    "    n_hypergraphs = input_shape[0] // max(S) # The number of hypergaphs that need to be constructed for each instance.\n",
    "    temporal_hypergraph_layer = TemporalHypergraphLayer(hyperedge_dim=input_shape[-2], roi_dim=input_shape[-1], attn_dim=64, n_hypergraphs=n_hypergraphs)\n",
    "    A_ts = temporal_hypergraph_layer(H_F_ts) # Here, the time dimension gets 'transformed' into the hyperedge dimension. Not sure how exactly this works. [Batch, Segment, ROIs, Hyperedges]\n",
    "\n",
    "    # Create a bipartite graph tensor for each segment and initialize node encodings.\n",
    "    H_F_ts_T = tf.transpose(H_F_ts, perm=[0, 1, 3, 2])  # [Batch, Segment, Time, ROIs] -> [Batch, Segment, ROIs, Time]\n",
    "    pooled_features = MessagePassingLayer()(A_ts, H_F_ts_T) # convert to bipartite graph tensor and initialise node encodings.\n",
    "\n",
    "    # Classification head\n",
    "    outputs = classification_head(pooled_features, n_classes)\n",
    "\n",
    "    return models.Model(inputs=input, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "n_classes = len(data_loader.unique_labels)\n",
    "n_rois = 360 # 360 ROIs defined in the Glasser parcellation\n",
    "input_shape = (data_loader.max_seq_len, n_rois) # [Time, ROIs]\n",
    "s_max=data_loader.s_max\n",
    "S = [5, 10, s_max] # Window sizes. All windows need to be a multiple of all smaller windows.\n",
    "\n",
    "model = build_model(input_shape=input_shape, ff_dim=64, n_classes=n_classes, S=S)\n",
    "\n",
    "# Create an Adam optimizer with the specified learning rate\n",
    "learning_rate = 1e-4\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Generated Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "Uncomment the callbacks to store logs for TensorBoard and store the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = '../logs/'\n",
    "\n",
    "train_data = data_loader.get_train()\n",
    "val_data = data_loader.get_val()\n",
    "test_data = data_loader.get_test()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(\n",
    "    train_data,  # a tf.data.Dataset object\n",
    "    epochs=100, \n",
    "    validation_data=val_data,  # A tf.data.Dataset object\n",
    "    callbacks=[  # Callbacks for early stopping, model checkpointing and TensorBoard\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "        # tf.keras.callbacks.ModelCheckpoint(filepath='best_model.keras', save_best_only=True),\n",
    "        # tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1, write_graph=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Accuracy and Loss\n",
    "Run this to plot the development of accuracy and loss in the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'][1:])\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f\"Test Loss: {np.round(test_loss, 2)}, Test Accuracy: {np.round(test_accuracy, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "## Generate a confusion matrix for train, val and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and generate confusion matrix\n",
    "# For Train Data\n",
    "train_predictions = model.predict(data_loader.get_train())\n",
    "train_pred_labels = np.argmax(train_predictions, axis=1)\n",
    "\n",
    "# For validation Data\n",
    "val_predictions = model.predict(data_loader.get_val())\n",
    "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# For Test Data\n",
    "test_predictions = model.predict(data_loader.get_test())\n",
    "test_pred_labels = np.argmax(test_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrices with placeholder labels\n",
    "train_cm = confusion_matrix(data_loader.train_labels, train_pred_labels)\n",
    "test_cm = confusion_matrix(data_loader.test_labels, test_pred_labels)\n",
    "val_cm = confusion_matrix(data_loader.val_labels, val_pred_labels)\n",
    "# Convert placeholder labels to actual label names\n",
    "actual_labels = data_loader.convert_labels(np.array(data_loader.unique_labels))\n",
    "\n",
    "# Create subplots for the confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "# Plotting confusion matrix for train data\n",
    "sns.heatmap(train_cm, annot=False, fmt=\"d\", xticklabels=actual_labels, yticklabels=actual_labels, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - Train Data')\n",
    "axes[0].set_ylabel('Actual Labels')\n",
    "axes[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "# Plotting confusion matrix for validation data\n",
    "sns.heatmap(val_cm, annot=False, fmt=\"d\", xticklabels=actual_labels, yticklabels=actual_labels, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix - Val Data')\n",
    "axes[1].set_ylabel('Actual Labels')\n",
    "axes[1].set_xlabel('Predicted Labels')\n",
    "\n",
    "# Plotting confusion matrix for test data\n",
    "sns.heatmap(test_cm, annot=False, fmt=\"d\", xticklabels=actual_labels, yticklabels=actual_labels, ax=axes[2])\n",
    "axes[2].set_title('Confusion Matrix - Test Data')\n",
    "axes[2].set_ylabel('Actual Labels')\n",
    "axes[2].set_xlabel('Predicted Labels')\n",
    "\n",
    "plt.suptitle('Confusion Matrices for Train, Validation, and Test Data')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More comprehensive training of the model\n",
    "## Running multiple combinations\n",
    "Run this to train the model on different combinations of window sizes and tasks. For further inspection, this generates an image containing confusion matrices for train, val and test data. Outputs a dictionary. Key is the used combination and values are a dictionary containing Accuracy, Training Time, path to the confusion matrix and a list containing the 3 confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different window sizes and tasks\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# make the dir confusion_matrices if it does not exist\n",
    "if not os.path.exists('./confusion_matrices'):\n",
    "    os.makedirs('./confusion_matrices')\n",
    "\n",
    "# Define the hyperparameter space\n",
    "window_sizes = [\n",
    "    [20],\n",
    "    [10, 20],\n",
    "    [5, 10, 20]\n",
    "]\n",
    "\n",
    "task_combinations = [\n",
    "    ['GAMBLING', 'SOCIAL'],\n",
    "    ['GAMBLING', 'SOCIAL', 'EMOTION'],\n",
    "    ['EMOTION', 'GAMBLING', 'RELATIONAL', 'SOCIAL'],\n",
    "    ['MOTOR', 'WM', 'EMOTION', 'GAMBLING', 'RELATIONAL', 'SOCIAL']\n",
    "]\n",
    "\n",
    "# Calculate the total number of combinations to inform progress\n",
    "total_combinations = len(task_combinations) * len(window_sizes)\n",
    "current_combination = 1  # Initialize the current combination counter\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "# Hyperparameters for the model\n",
    "learning_rate = 1e-4\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "n_subjects = N_SUBJECTS  \n",
    "hcp_dir = HCP_DIR \n",
    "run_direction = RUN_DIRECTION['RL'] \n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for tasks in task_combinations:\n",
    "    for S in window_sizes:\n",
    "        print(f'Testing combination {current_combination} / {total_combinations}: Tasks {tasks}, Window Sizes {S}')\n",
    "\n",
    "        config_id = f'Tasks: {\"_\".join(tasks)}, Windows: {\"_\".join(map(str, S))}'\n",
    "\n",
    "        # Load data\n",
    "        data_loader = DataLoader(batch_size=batch_size, tasks=tasks, subjects=range(n_subjects), dir=hcp_dir, run_direction=run_direction, s_max=max(S), random_state=42)\n",
    "        train_data = data_loader.get_train()\n",
    "        val_data = data_loader.get_val()\n",
    "        test_data = data_loader.get_test()\n",
    "        \n",
    "        # Build the model\n",
    "        input_shape = (data_loader.max_seq_len, n_rois)  # [Time, ROIs]\n",
    "        n_classes = len(data_loader.unique_labels)\n",
    "        model = build_model(input_shape=input_shape, ff_dim=64, n_classes=n_classes, S=S)\n",
    "        \n",
    "        # Compile the model\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Fit the model\n",
    "        model.fit(train_data, epochs=epochs, validation_data=val_data, verbose=2, callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "        ])\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Evaluate the model\n",
    "        test_loss, test_acc = model.evaluate(test_data)\n",
    "        \n",
    "        # Predict and generate confusion matrix\n",
    "        # For Train Data\n",
    "        train_predictions = model.predict(data_loader.get_train())\n",
    "        train_pred_labels = np.argmax(train_predictions, axis=1)\n",
    "\n",
    "        # For validation Data\n",
    "        val_predictions = model.predict(data_loader.get_val())\n",
    "        val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "        # For Test Data\n",
    "        test_predictions = model.predict(data_loader.get_test())\n",
    "        test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "        # proceed to compute the confusion matrix using the predictions made\n",
    "        train_cm = confusion_matrix(data_loader.train_labels, train_pred_labels)\n",
    "        test_cm = confusion_matrix(data_loader.test_labels, test_pred_labels)\n",
    "        val_cm = confusion_matrix(data_loader.val_labels, val_pred_labels)\n",
    "        actual_labels = data_loader.convert_labels(np.array(data_loader.unique_labels))\n",
    "        # Save the confusion matrix as an image\n",
    "\n",
    "        # Create subplots for the confusion matrices\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "        # Plotting confusion matrix for train data\n",
    "        sns.heatmap(train_cm, annot=False, fmt=\"d\", xticklabels=actual_labels, yticklabels=actual_labels, ax=axes[0])\n",
    "        axes[0].set_title('Confusion Matrix - Train Data')\n",
    "        axes[0].set_ylabel('Actual Labels')\n",
    "        axes[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "        # Plotting confusion matrix for validation data\n",
    "        sns.heatmap(val_cm, annot=False, fmt=\"d\", xticklabels=actual_labels, yticklabels=actual_labels, ax=axes[1])\n",
    "        axes[1].set_title('Confusion Matrix - Val Data')\n",
    "        axes[1].set_ylabel('Actual Labels')\n",
    "        axes[1].set_xlabel('Predicted Labels')\n",
    "\n",
    "        # Plotting confusion matrix for test data\n",
    "        sns.heatmap(test_cm, annot=False, fmt=\"d\", xticklabels=actual_labels, yticklabels=actual_labels, ax=axes[2])\n",
    "        axes[2].set_title('Confusion Matrix - Test Data')\n",
    "        axes[2].set_ylabel('Actual Labels')\n",
    "        axes[2].set_xlabel('Predicted Labels')\n",
    "\n",
    "        plt.suptitle(f'Confusion Matrix for tasks: {tasks} and window sizes: {S}')\n",
    "        plt.savefig(f'./confusion_matrices/cm_tasks_{\"_\".join(tasks)}_windows_{\"_\".join(map(str, S))}.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Log the accuracy and training time\n",
    "        print(f'Tasks: {tasks}, Window Sizes: {S}, Accuracy: {test_acc}, Training Time: {training_time}')\n",
    "        # Collect results\n",
    "        results[config_id] = {\n",
    "            'Accuracy': test_acc,\n",
    "            'Training Time': (time.time() - start_time) / 60,\n",
    "            'Confusion Matrix Image Path': f'./confusion_matrices/cm_{config_id.replace(\", \", \"_\").replace(\": \", \"-\")}.png',\n",
    "            'Confusion Matrices': [train_cm, val_cm, test_cm]\n",
    "        }\n",
    "        current_combination += 1  # Move to the next combination\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the Output of Layers\n",
    "\n",
    "## Example: Output of the temporal hypergraph layer\n",
    "\n",
    "Note that the number at the end of the layer name changes. To work, this needs to be adjusted. The current number can be found in the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temoral_hypergraph_layer_name = 'temporal_hypergraph_layer_25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get single element from batch\n",
    "batch = test_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model for inspection that outputs the adjacency matrix\n",
    "inspection_model = models.Model(inputs=model.input, outputs=model.get_layer(temoral_hypergraph_layer_name).output)\n",
    "\n",
    "# Predict with the inspection model to get the adjacency matrix\n",
    "adjacency_matrices = inspection_model.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjacency_matrices.shape)\n",
    "# get single element from batch\n",
    "first = adjacency_matrices[0]\n",
    "print(first.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the visualisation assumes 4 adjacency matrices. This means the maximum time series length T / S_MAX should be 4 to generate 4 hypergraph segments in the temporal hypergraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the adjacency matrix into the different segments of size 40 x 360\n",
    "n_segments = first.shape[0]\n",
    "segments = [first[i] for i in range(n_segments)]\n",
    "\n",
    "# binarise each segment\n",
    "binary_segments = [np.where(segment >= 1, 1, 0) for segment in segments]\n",
    "\n",
    "# Visualise all 4 segments with time step as label\n",
    "fig_size = (15, 10)  # Adjust the size of the figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=fig_size)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    pos = ax.imshow(binary_segments[i], cmap='viridis', aspect='auto', interpolation='none')\n",
    "    ax.set_title(f'Segment {i+1}', pad=20)\n",
    "\n",
    "# Add a color bar\n",
    "fig.colorbar(pos, ax=axs, orientation='horizontal', fraction=0.02, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a hypergraph reprentation, use hypernetx to create hypergraph objects and then visualise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate hypergraphs for each segment\n",
    "hypergraphs = []\n",
    "for i, segment in enumerate(binary_segments):\n",
    "    hyperedges = []\n",
    "    for j in range(segment.shape[0]):\n",
    "        hyperedges.append(set(np.where(segment[j])[0]))\n",
    "\n",
    "    H = hnx.Hypergraph(hyperedges)\n",
    "    hypergraphs.append(H)\n",
    "hypergraphs = [graph.dual().collapse_nodes_and_edges() for graph in hypergraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of rows and columns for subplot grid\n",
    "# 2x2 grid for 4 hypergraphs\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(12, 12)) \n",
    "\n",
    "# Flatten the array of axes if necessary\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each hypergraph in its subplot\n",
    "for i, (graph, ax) in enumerate(zip(hypergraphs, axs)):\n",
    "    hnx.draw(graph, ax=ax)\n",
    "    ax.set_title(f\"Segment {i+1}\")\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sthop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
